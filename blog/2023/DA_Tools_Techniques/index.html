<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="numpy">NumPy</h1> <ul> <li> <p>Python provides built-in libraries for both multi-threading and multi-processing, with high level abstractions</p> </li> <li>NumExpr and NumPy can speed-up Python codes through vectorisation and multithreading, Numba can even make use of OpenMP threading, if installed.</li> <li>Some libraries offer a “<strong>njobs</strong>” option, such as sci-kit learn, which uses multiprocessing to parallelize ML training, though scaling is limited in practice.</li> <li>The Python GIL prevents true multi-threaded parallelism <ul> <li>However, each Python process has its own GIL.</li> <li>NumExpr and NumPy compile to native code, so avoid dealing with the GIL.</li> </ul> </li> </ul> <h1 id="apache-spark">Apache Spark</h1> <ul> <li> <strong>Apache Spark</strong> is an engine for executing data engineering, data science, and machine learning on single-node machines or clusters (单机或集群都可以)</li> <li>Key features: <ul> <li>Streaming data</li> <li>Analytics</li> <li>Data science at Scale</li> <li>Machine Learning</li> </ul> </li> <li>Clusters: <ul> <li>Component: ① Driver Program ② Cluster Manager ③ Worker Nodes</li> <li><img src="images/image-20230303000427295.png" alt="image-20230303000427295" style="zoom:50%;"></li> <li>Supported cluster manager: ① Hadoop YARN ② Apache Mesos ③ Kubernetes</li> </ul> </li> </ul> <h3 id="why-apache-spark">Why Apache Spark</h3> <ul> <li>Focuses on computation. In-memory storage for intermediate computations</li> <li>APIs for multiple programming languages. Java Scala Python SQL R</li> <li>Libraries for different type of workflows.</li> </ul> <h3 id="how-spark-operates">How Spark operates?</h3> <ul> <li> <strong>Transformations</strong> create a new Spark data frame without altering the original one. They can additionally be divided into: <ul> <li> <strong><em>Narrow transformations</em></strong>, take a single input partition and return a single output partition (e.g. <em>filter operation</em>).</li> <li> <strong><em>Wide transformations</em></strong>, use multiple partitions and require reshuffling(重新洗牌) of the data (e.g. <em>groupby operation</em>).</li> </ul> </li> <li> <strong>Actions</strong> are eagerly executed, and produce a value (e.g., the result of processing a series of <strong>Transformations</strong>). Example actions are “count”, “collect”, and “saveAsTextFile”</li> </ul> <h3 id="spark-data-api">Spark Data API</h3> <ul> <li> <strong>Resilient Distributed Dataset (RDD)</strong> <ul> <li>General purpose.</li> <li>==<strong>Represents data partitioned across cluster, operated on in parallel.</strong>==</li> <li> <strong>Transparently</strong> moves data around the cluster</li> <li>More suitable for ==<strong>unstructured</strong>== data</li> <li>Less optimized</li> <li>==<strong>Immutable</strong>.== Computation arranged as graph of RDD transformations, each <strong>creating a new one</strong> </li> <li>Persisted <strong>in memory</strong>(存在内存中) so lower disk I/O requirements</li> <li>Supports reconstruction of lost data from source (efficient based on knowledge of computation graph)</li> </ul> </li> <li> <strong>DataFrame</strong> <ul> <li>Tabular</li> <li>Schema based</li> <li>More suitable for==<strong>structured data</strong>==</li> <li><strong>More optimized</strong></li> </ul> </li> <li> <strong>Dataset</strong> <ul> <li>Combination of both</li> </ul> </li> </ul> <h3 id="spark-ml-library">Spark ML Library</h3> <ul> <li> <p>Spark makes possible to run Machine Learning workflows using their Spark ML library. This library can be divided into 4 key constituents:</p> <ul> <li> <strong>Transformers</strong>: are mainly used to perform data engineering/pre-processing tasks (e.g.scaling, feature selection, etc…) and they just apply rule-based transformations (there is no learning from data). They take a <strong>data frame</strong> as input and return <strong>a new data frame</strong>.</li> <li> <p><strong>Estimators</strong>: learn parameters from the data and return a trained ML model (which is a Transformer).</p> </li> <li> <strong>Pipeline</strong>: organizes a sequence of Transformers and Estimators in a single object.</li> <li> <strong>Evaluators</strong>: used in order to assess our Machine Learning models through various classification, regression, etc… metrics.</li> </ul> </li> </ul> </body></html>