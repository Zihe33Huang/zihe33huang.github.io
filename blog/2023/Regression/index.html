<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="linear-regression">Linear Regression</h1> <h2 id="types-of-machine-learning">Types of Machine Learning</h2> <ul> <li> <strong>Supervised Learning</strong>: where there is an input <em>X</em>, and an output <em>Y</em> and the task is to learn the mapping from the input to the output <ul> <li> <strong>Classification</strong>: when <em>Y</em> is a <strong>categorical</strong> variable (e.g. spam/not spam)</li> <li> <strong>Regression</strong>: when <em>Y</em> is a <strong>continuous</strong> variable.</li> </ul> </li> <li> <strong>Unsupervised Learning:</strong> there is only input <em>X</em>. The aim is to find regularities/structure in the input space. One method is called <em>clustering</em>, where the aim is to find clusters or groupings of input.</li> </ul> <h2 id="linear-regression-1">Linear Regression</h2> <p><img src="images/image-20230302224557667.png" alt="image-20230302224557667"></p> <h4 id="the-model">The model</h4> <p>In regression, we would like to write the numeric <strong>output</strong>, called the ==<strong>dependent or response variable</strong>==, as a function of the <strong>input</strong>, called the ==<strong>independent or explanatory variable</strong>==(also known as features).</p> <h4 id="cost-function">Cost Function</h4> <ul> <li>Least squares estimation</li> <li> <p><img src="images/image-20230302225720066.png" alt="image-20230302225720066" style="zoom:33%;"></p> </li> <li> <p><img src="images/image-20230302225742613.png" alt="image-20230302225742613" style="zoom: 25%;"></p> </li> <li>Find the values of θ0 and θ1 which have minimum cost.</li> <li>θ0 is the intercept(截距) ; also referred to as bias in machine learning literature.</li> <li>θ1 is the slope of the line.</li> </ul> <h2 id="learning-process-for-supervised-learning">Learning Process for Supervised learning</h2> <ul> <li> <p>Define model: <img src="images/image-20230302230125731.png" alt="image-20230302230125731" style="zoom:25%;"></p> </li> <li> <p>Build the cost function:<img src="images/image-20230302230149973.png" alt="image-20230302230149973" style="zoom:25%;"></p> </li> <li> <p>Optimize the cost function to find the model’s parameters</p> <ul> <li> <p>Optimization algorithms: ① <strong>Gradient</strong>(斜率, 梯度) descent [梯度下降法] ② <strong>stochastic</strong>(随机) gradient descent.</p> </li> <li> <p>Normal equation (in linear and multiple regression)</p> </li> </ul> </li> <li> <p>Evaluation of the model</p> </li> </ul> <h4 id="gradient-descent-optimization">Gradient descent optimization</h4> <ul> <li> <p>Start with any initial values for parameters</p> </li> <li> <p>Changing parameters to reduce the cost</p> </li> <li> <p>Repeat until at the minimum</p> </li> <li> <p>We use the gradient to change parameters in the right direction so that cost is reduced.</p> <p><img src="images/image-20230302231142879.png" alt="image-20230302231142879" style="zoom:25%;"></p> </li> </ul> <h4 id="gradient-descent-algorithm梯度下降法">Gradient descent algorithm(梯度下降法)</h4> <ul> <li>Parameter: θ<sub>0</sub> , θ<sub>1</sub> </li> <li>Calculate: h(x) = θ<sub>0 </sub> + θ<sub>1</sub> x</li> <li> <p>The cost function is <img src="images/image-20230302231711362.png" alt="image-20230302231711362" style="zoom:25%;"></p> </li> <li> <p>Repeat until <strong>convergence</strong>(收敛) <img src="images/image-20230302231840533.png" alt="image-20230302231840533" style="zoom:25%;"></p> <p>All parameters are updated simultaneously</p> </li> <li> <p>α is the learning rate and controls the size of the steps.</p> </li> <li> <p>Linear regression partial derivatives (偏导数)</p> </li> <li> <p><strong>Property</strong>:</p> <ul> <li>Batch algorithm because each iteration looks at all the training data</li> <li>Simultaneously update parameters according to partial derivatives</li> <li>Can be slow to converge</li> <li>Each iteration is slow processing all the training data</li> <li>Feature scaling is a good idea</li> <li><strong>Easily vectorized to use parallel linear algebra libraries that can exploit parallelism</strong></li> </ul> </li> </ul> <h4 id="stochastic-gradient-descent-algorithm随机梯度下降法">Stochastic gradient descent algorithm(随机梯度下降法)</h4> <ul> <li> <p>Randomly shuffle training data</p> </li> <li> <p>repeat {</p> <p>​ for {</p> <p><img src="images/image-20230302233125159.png" alt="image-20230302233125159" style="zoom:25%;"></p> <p>​ }</p> <p>}</p> </li> <li> <p>Can scale algorithms to much bigger training sets</p> </li> <li> <p>May require few passes through training set (~1 to10)</p> </li> <li> <p>Very little scope for parallelism</p> </li> </ul> <h4 id="mini-batch-gradient-descent">Mini-batch gradient descent</h4> <p>Hybrid between batch (update after all <em>m</em> records) and stochastic (update after 1 record)</p> <ul> <li> <p>Upgrade using batch algorithm every <em>b&lt;m</em> training records</p> </li> <li> <p>Supports exploitation of vectorization and hence parallelism</p> </li> </ul> <h4 id="normal-equations">Normal equations</h4> <ul> <li> <p>Analytic solution rather than iterative</p> </li> <li> <em>n</em> features, <em>m</em> training records</li> <li> <p><img src="images/image-20230302233618395.png" alt="image-20230302233618395" style="zoom: 50%;"></p> </li> <li>X is called the design matrix</li> <li>Cost Function: <img src="images/image-20230302234813426.png" alt="image-20230302234813426" style="zoom:33%;"> <ul> <li>Firstly, representing cost function as a matrix form: <img src="images/image-20230302234913841.png" alt="image-20230302234913841" style="zoom:33%;"> </li> <li>Simplify: <img src="images/image-20230302234943868.png" alt="image-20230302234943868" style="zoom:33%;"> </li> <li>Now make use of multivariate calculus to find the partial derivative and make it equal to 0 in all directions. Then, we can get the θ we want.</li> </ul> </li> <li>https://zhuanlan.zhihu.com/p/60719445 推导过程</li> </ul> <h4 id="comparison-of-gradient-descent-and-normal-equation">Comparison of Gradient descent and normal equation</h4> <ul> <li> <em>n</em> features, <em>m</em> training records</li> <li>Gradient descent <ul> <li>Need to choose α</li> <li>Iterative</li> <li>Works for large <em>n</em> </li> <li>Used with many learning algorithms</li> </ul> </li> <li>Normal equation <ul> <li>No need to choose α</li> <li>No iterations required</li> <li>Inverting <em>n × n</em> matrix can be slow for large <em>n</em> </li> <li>Only works with linear regression</li> </ul> </li> </ul> </body></html>